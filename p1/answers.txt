
Q2
Prediction:
  Latency:
    h1 to h4(avg rtt) = latency_l1 + latency_l2 + latency_l3 = 170.049 ms
  Throughput:
    min_server_throughput = 18.94 Mbps
    min_client_throughput = 22.90 Mbps
    h1 to h4 = min(throughput_l1, throughput_l2, throughput_l3) = 18.94 Mbps 

Measurement:
  Latency:
    h1 to h4(avg rtt) = 168.037 ms
  Throughput:
    server = 18.27 Mbps
    client = 22.64 Mbps
    h1 to h4 = 18.27 Mbps

Analysis:
  We predicted the time it takes for data to go back and forth (latency) between certain points in the network 
  by adding up the latencies for three specific links (L1, L2, and L3), expecting an average of 170.049 milliseconds (ms). 
  The actual time measured was a bit less, at 168.037 ms. This small difference of about 2 ms might be because of 
  how precise our tools were, changes in the network, or delays in the network equipment that we didn't factor into our prediction. 
  Nevertheless, the prediction was very close to the actual measurement, showing our prediction method was pretty accurate.

  For the data transfer speed (throughput) from point h1 to h4, we predicted the slowest link in the path 
  (among L1, L2, and L3) would set the maximum speed, which we thought would be 18.94 Megabits per second (Mbps). 
  The real speed measured was a bit slower at 18.27 Mbps. This slight drop in speed might be due to extra delays 
  we didn't think about, like those from network protocols, or changes in how busy the network was when we measured. 
  However, our prediction was very close to the real speed, showing that our method for figuring out the slowest 
  part of the path was accurate.


******************************************************************************************************************************************************

Q3

Two pairs:
  Prediction:   
    Latency:
      h1 to h4(avg rtt) = 170.049 ms
      h7 to h9(avg rtt) = 170.049 ms
    Throughput:
      h1 to h4 = 18.94 / 2 = 9.47 Mbps
      h7 to h9 = 18.94 / 2 = 9.47 Mbps

  Measure:
    Latency:
      h1 to h4(avg rtt) = 168.219 ms
      h7 to h9(avg rtt) = 172.611 ms
    Throughput:
      server (h1) = 12.50 Mbps
      client (h4) = 17.50 Mbps
      server (h7) = 6.59 Mbps
      client (h9) = 8.70 Mbps
      h1 to h4 = 12.5 Mbps
      h7 to h9 = 6.59 Mbps
  
Three pairs:
  Prediction:
    Latency:
      h8 to h10(avg rtt) = 170.049 ms
      h7 to h9(avg rtt) = 170.049 ms
      h1 to h4(avg rtt) = 170.049 ms
    Throughput:
      h8 to h10 = 18.94 / 3 = 6.31 Mbps
      h7 to h9 = 18.94 / 3 = 6.31 Mbps
      h1 to h4 = 18.94 / 3 = 6.31 Mbps

  Measure:
    Latency:
      h8 to h10(avg rtt) = 168.727 ms
      h7 to h9(avg rtt) = 170.935 ms
      h1 to h4(avg rtt) = 170.029 ms  
    Throughput:
      server (h1) = 3.26 Mbps
      client (h4) = 4.11 Mbps
      server (h8) = 5.35 Mbps
      client (h10) = 7.05 Mbps
      server (h7) = 11.00 Mbps
      client (h9) = 13.76 Mbps
      h4 to h1 = 3.26 Mbps
      h10 to h8 = 5.35 Mbps 
      h9 to h7 = 11.00 Mbps

Analysis:
  The latency, measured as the average round-trip time (RTT), remains relatively stable across both scenarios 
  (two pairs and three pairs of hosts). This stability suggests that the network can handle multiple simultaneous 
  connections without significant increases in latency. The slight variations in RTT (e.g., from 168.219 ms to 172.611 ms in the two-pairs scenario) 
  could be attributed to network congestion or variability in packet routing paths, but overall, the impact on latency is minimal.

  The throughput analysis reveals more significant effects due to multiplexing. When two pairs of hosts are communicating, 
  the throughput does not simply halve for each pair compared to the single-pair scenario; instead, there's an unequal split. 
  For instance, one pair (h1 to h4) achieves a higher throughput (12.5 Mbps) than predicted (9.47 Mbps), 
  while the other (h7 to h9) sees a decrease to 6.59 Mbps. This discrepancy might result from the network's dynamic response to congestion, 
  where certain paths may gain preferential bandwidth allocation based on current network conditions or configurations.


******************************************************************************************************************************************************

Q4
Prediction:
  Latency:
    h1 to h4(avg rtt) = 170.049 ms
    h5 to h6(avg rtt) = latency_l4 + latency_l2 + latency_l5 = 11.911 + 22.657 + 11.897 = 46.465 ms
  Throughput:
    h1 to h4 = 18.94 Mbps 
    h5 to h6 = 23.90 Mbps

Measure:
  Latency:
    h1 to h4(avg rtt) = 170.624 ms
    h5 to h6(avg rtt) = 48.039 ms
  Throughput:
    h1 to h4 = 15.08 Mbps
    h5 to h6 = 23.03 Mbps

Analysis:
  For h1 to h4, we almost hit the mark with our guess, expecting 170.049 ms and seeing 170.624 ms. This tiny difference 
  shows our guess was really close. For h5 to h6, our guess was 46.465 ms, and the actual was a bit higher at 48.039 ms. 
  Again, we were pretty close, showing we understand how delays add up in the network.

  The data speed (throughput) from h1 to h4 was lower than we thought. We guessed 18.94 Mbps, but it was actually 15.08 Mbps.
  This drop might be because the network was busier than we expected. For h5 to h6, our guess was close again. 
  We predicted 23.90 Mbps, and it came out to 23.03 Mbps. This small miss suggests our method for guessing speeds is 
  on the right track but might need a tweak for busy networks.



